##put recall and JOLs on the same scale
colnames(nomiss2)[7] = "recall"
nomiss2$Recall.Correct = (nomiss2$Recall.Correct * 100)
colnames(nomiss2)[8] = "Recall"
##melt the data
long.dat = melt(nomiss2, id = c("Subject", "block",
"ListNum", "direction", "ExperimentName", "cue_target",
"ListNum", "Procedure.Trial.", "recall", "Recall_Prompt"))
summary(long.dat)
colnames(long.dat)[11] = "Task"
colnames(long.dat)[12] = "Score"
summary(long.dat)
bar1 = ggplot(long.dat, aes(direction, Score, fill = Task))
bar1 = bar1 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("JOL" = "white",
"Recall" = "dimgray")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance") +
ylim(0,100)
bar1
####get descriptives####
nomiss = read.csv("final data.csv")
##add block
nomiss$block = c(rep(1, 80), rep(2, 80))
##put recall and jols on same scale
nomiss$Recall = (nomiss$Recall * 100)
##get number of subjects
length(unique(unlist(nomiss[c("Subject")])))
##means
meanJol = tapply(nomiss$Jol, nomiss$Direction, mean)
meanR = tapply(nomiss$Recall, nomiss$Direction, mean)
#sds
sdJol = tapply(nomiss$Jol, nomiss$Direction, sd, na.rm = T)
sdR = tapply(nomiss$Recall, nomiss$Direction, sd, na.rm = T)
##n
nJol = tapply(nomiss$Jol, nomiss$Direction, length)
nR = tapply(nomiss$Recall, nomiss$Direction, length)
####ANOVAS####
##JOL ANALYSIS
model1 = ezANOVA(data = nomiss,
wid = Subject,
within = Direction,
dv = Jol,
detailed = TRUE)
model1
##calculate MSE for JOLS
jol.mse = model1$ANOVA$SSd[2]/model1$ANOVA$DFd[2]
##RECALL ANALYSIS
model2 = ezANOVA(data = nomiss,
wid = Subject,
within = Direction,
dv = Recall,
detailed = TRUE)
model2
##calculate MSE for Recall
r.mse = model2$ANOVA$SSd[2]/model2$ANOVA$DFd[2]
jol.mse;r.mse
##get dfn (mean)
dfn.jol = model1$ANOVA$DFn[2]
dfn.r = model2$ANOVA$DFn[2]
##get dfd (error)
dfd.jol = model1$ANOVA$DFd[2]
dfd.r = model2$ANOVA$DFd[2]
##get ssn (mean)
ssn.jol = model1$ANOVA$SSn[2]
ssn.r = model2$ANOVA$SSn[2]
##get ssd (error)
ssd.jol = model1$ANOVA$SSd[2]
ssd.r = model2$ANOVA$SSd[2]
##get F
fstat.jol = model1$ANOVA$F[2]
fstat.r = model2$ANOVA$F[2]
####post hoc t-tests####
##JOL
pairwise.t.test(nomiss$Jol, nomiss$Direction, ##significant difference between Unrelated pairs and related pairs
paired = F, p.adjust.method = 'none')
##Recall
pairwise.t.test(nomiss$Recall, nomiss$Direction, ##all are significant
paired = F, p.adjust.method = 'none')
##get t values for post hocs
source("pairwise_t.R")
Jol.t = pairwise.t.test.with.t.and.df(nomiss$Jol, nomiss$Direction,
paired = F, p.adjust.method = 'none')
Jol.t
print(Jol.t)[[5]] #T-values
print(Jol.t)[[6]] #df
R.t = pairwise.t.test.with.t.and.df(nomiss$Recall, nomiss$Direction,
paired = F, p.adjust.method = 'none')
R.t
print(R.t)[[5]] #T-values
print(R.t)[[6]] #df
model1
model2
meanJol = tapply(nomiss$Jol, nomiss$Direction, mean)
meanR = tapply(nomiss$Recall, nomiss$Direction, mean)
meanJol
sdJol
meanR = tapply(nomiss$Recall, nomiss$Direction, mean)
sdR = tapply(nomiss$Recall, nomiss$Direction, sd, na.rm = T)
sdR
meanR
nJol = tapply(nomiss$Jol, nomiss$Direction, length)
nR = tapply(nomiss$Recall, nomiss$Direction, length)
meanR;sdR;nR
jfb = d.dep.t.avg(68.20625, 66.08839, 28.41285, 28.81500, 1120, a = .05) #d = .07
jfb.d = round(jfb$d, digits = 2)
jfb.d
print(R.t)[[5]] #T-values
pairwise.t.test(nomiss$Jol, nomiss$Direction, ##significant difference between Unrelated pairs and related pairs
paired = F, p.adjust.method = 'Bonferroni')
pairwise.t.test(nomiss$Jol, nomiss$Direction, ##significant difference between Unrelated pairs and related pairs
paired = F, p.adjust.method = 'bonferroni')
pairwise.t.test(nomiss$Recall, nomiss$Direction, ##all are significant
paired = F, p.adjust.method = 'bonferroni')
Jol.t = pairwise.t.test.with.t.and.df(nomiss$Jol, nomiss$Direction,
paired = F, p.adjust.method = 'bonferroni')
R.t = pairwise.t.test.with.t.and.df(nomiss$Recall, nomiss$Direction,
paired = F, p.adjust.method = 'bonferroni')
print(Jol.t)[[5]] #T-values
print(R.t)[[5]] #T-values
rfs = d.dep.t.avg(67.40741, 58.83929, 46.89169, 49.23445, 1120, a = .05) #d = 0.18
rfs.d = round(rfs$d, digits = 2)
rfs.d
model9 = aov(int.dat$diff ~ (int.dat$jol_bin*int.dat$direction))
nomiss.f = subset(nomiss,
nomiss$Direction == "F")
nomiss.b = subset(nomiss,
nomiss$Direction == "B")
nomiss.s = subset(nomiss,
nomiss$Direction == "S")
nomiss.u = subset(nomiss,
nomiss$Direction == "U")
##get rounded jols
nomiss.f$jol_bin = round(nomiss.f$Jol, -1)
nomiss.b$jol_bin = round(nomiss.b$Jol, -1)
nomiss.s$jol_bin = round(nomiss.s$Jol, -1)
nomiss.u$jol_bin = round(nomiss.u$Jol, -1)
##put data in wide format
##jol
f.jol = cast(nomiss.f, Subject ~ block, mean, value = 'Jol')
f.jol$mean_JOL = apply(f.jol, 1, mean)
f.jol$jol_bin = round(f.jol$mean_JOL, -1)
b.jol = cast(nomiss.b, Subject ~ block, mean, value = 'Jol')
b.jol$mean_JOL = apply(b.jol, 1, mean)
b.jol$jol_bin = round(b.jol$mean_JOL, -1)
s.jol = cast(nomiss.s, Subject ~ block, mean, value = 'Jol')
s.jol$mean_JOL = apply(s.jol, 1, mean)
s.jol$jol_bin = round(s.jol$mean_JOL, -1)
u.jol = cast(nomiss.u, Subject ~ block, mean, value = 'Jol')
u.jol$mean_JOL = apply(u.jol, 1, mean)
u.jol$jol_bin = round(u.jol$mean_JOL, -1)
##recall
f.recall = cast(nomiss.f, Subject ~ block, mean, value = 'Recall')
f.recall$mean_recall = apply(f.recall, 1, mean)
b.recall = cast(nomiss.b, Subject ~ block, mean, value = 'Recall')
b.recall$mean_recall = apply(b.recall, 1, mean)
s.recall = cast(nomiss.s, Subject ~ block, mean, value = 'Recall')
s.recall$mean_recall = apply(s.recall, 1, mean)
u.recall = cast(nomiss.u, Subject ~ block, mean, value = 'Recall')
u.recall$mean_recall = apply(u.recall, 1, mean)
##add direction column
f.recall$direction = rep("f", 28)
f.jol$direction = rep("f", 28)
b.recall$direction = rep("b", 28)
b.jol$direction = rep("b", 28)
s.recall$direction = rep("s", 28)
s.jol$direction = rep("s", 28)
u.recall$direction = rep("u", 28)
u.jol$direction = rep("u", 28)
##Put everything back together
int.recall = rbind(f.recall, b.recall, s.recall, u.recall)
int.jol = rbind(f.jol, b.jol, s.jol, u.jol)
int.recall = int.recall[ , -c(2:3)]
int.jol = int.jol[ , -c(2:3)]
int.dat = cbind(int.jol, int.recall)
int.dat = int.dat[ , -c(4:5)]
##get difference score
int.dat$mean_recall = int.dat$mean_recall * 100
int.dat$diff = int.dat$jol_bin - int.dat$mean_recall
library(car)
model9 = aov(int.dat$diff ~ (int.dat$jol_bin*int.dat$direction))
summary(model9)
library(lsr)
etas1 = etaSquared(model9)
bar3 = ggplot(long.dat, aes(Direction, Score, fill = Task))
bar3 = bar3 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("Jol" = "white",
"Recall" = "dimgrey")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance")
ylim(0,100)
#labs(title="All Blocks")
bar3
dat = read.csv("ex2 final output.csv")
summary(dat)
library(ggplot2)
library(reshape)
##put recall on correct scale
dat$Scored_Response = (dat$Scored_Response * 100)
##remove out of range scores
dat$Jol_Response[dat$Jol_Response > 100] = NA
##get sample size
summary(dat$Subject) #n = 34
summary(dat)
##remove missing
nomiss3 = na.omit(dat)
colnames(nomiss3)[6] = "Jol"
colnames(nomiss3)[9] = "Recall"
####make the graph####
##melt the data
long.dat = melt(nomiss3, id = c("Subject", "Block",
"ListNum", "Direction", "ExperimentName", "cue_target",
"recall_response", "cue_prompt"))
summary(long.dat)
colnames(long.dat)[9] = "Task"
colnames(long.dat)[10] = "Score"
bar3 = ggplot(long.dat, aes(Direction, Score, fill = Task))
bar3 = bar3 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("Jol" = "white",
"Recall" = "dimgrey")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance")
ylim(0,100)
#labs(title="All Blocks")
bar3
bar3 = ggplot(long.dat, aes(Direction, Score, fill = Task))
bar3 = bar3 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("Jol" = "white",
"Recall" = "dimgrey")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance") +
ylim(0,100)
#labs(title="All Blocks")
bar3
knitr::include_graphics("plot1.png")
citr:::insert_citation()
12.11*40
484.4*2
968.8*.15
968.8-145.32
823-110-50
85-22)
85-22
n = 100
dbinom(100)
dbinom(50, 100, 50)
dbinom(50, 100, 2)
dbinom(50, 100, .5)
dbinom(50, 100, .5)
round(data.frame(0:100, probs), digits = 5)
probs = dbinom(50, 100, .5)
round(data.frame(0:100, probs), digits = 5)
plot(0:100, probs, type="h", xlim=c(0,100), ylim=c(0,.1))
probs = dbinom(0:100, 100, .5) ##get the probability
round(data.frame(0:100, probs), digits = 5)
plot(0:100, probs, type="h", xlim=c(0,100), ylim=c(0,.1))
points(0:100, probs, pch=16, cex=.5)
curve(dnorm(x, mean=50, sd=5), from=0, to=100, xlim = c(0, 100), ylim = c(0, 0.5), xlab = "x", add=T, col="blue")
sum(dbinom(45:55, size=100, prob=1/2))
sum(dbinom(50, size=100, prob=1/2))
length(probs)
probs = as.data.frame(probs)
subset(probs, Mod(probs$probs) == 0)
View(probs)
head = 1
tail = 0
prob = e
e
e = exp(1)
sample(c("Heads", "Tails"), n, rep = T)
Flip1Coin = function(n) sample(c("Heads", "Tails"), n, rep = T)
Flip1Coin(n)
sample(c("Heads", "Tails"), n, rep = T)
sample(c("Heads", "Tails"), n, rep = 100)
Flip1Coin = function(n) sample(c("Heads", "Tails"), n, rep = 100)
Flip1Coin(n)
sample(c("Heads", "Tails"), n, rep = 100)
Flip1Coin = sample(c("Heads", "Tails"), n, rep = 100)
pbinom(50, 100, .5)
dbinom(50, 100, .5)
dnorm(50)
dnorm(50/sqrt(24))
pbinom(50, size = 100, .5)
pbinom(48, size = 100, .5)
pbinom(52, size = 100, .5)
pbinom(71, size = 100, .5)
m = 100 * .5
sd1 = sqrt(100 * .5 *.5)
1 - dnorm(50, mean = m, sd = sd1)
1 - pnorm(50, mean = m, sd = sd1)
x = c(0:100)
length(x)
mod(x, 2)
Mod(x)
Mod(x, 2)
.5*10
.75*7.5
.6*7.5
.7*7.5
.65*7.5
.68*7.5
.625*7.5
.667*7.5
.667*5
.667*4
.667*4.25
.667*8
.6 *8
.62 *8
.63 *8
.625 *8
.625 * 4
25*60
1500/5
install.packages("installr")
installr::installr()
library(ez)
library(reshape)
devtools::install_github("npm27/lrd")
devtools::install_github("npm27/lrd")
devtools::install_github("npm27/lrd")
shiny::runApp('GitHub/lrd/shiny')
setwd("~/GitHub/lrd-analysis/Sentences")
####Validate the sentence functions####
library(lrd)
dat = read.csv("AzBio_ScoreTemp_AH.csv")
View(dat)
#isolate the participant ids, sentences, and responses
dat2 = dat[ , c(1, 3, 4)]
#how many participants?
length(unique(dat2))
#how many participants?
length(unique(dat2$Participant.Private.ID))
#how many sentences?
length(unique(dat2$sentence))
21*101
2057/101
##score the things!
##make useful column names
colnames(dat2)[1] = "id"
##Need to make a trial id
#first, remove the blank sentences
dat2 = subset(dat2,
dat2$sentence != " ")
##Need to make a trial id
#first, remove the blank sentences
dat2 = subset(dat2,
dat2$sentence != "")
View(dat2)
#how many participants?
length(unique(dat2$Participant.Private.ID)) #101
#how many sentences?
length(unique(dat2$sentence)) #21
#how many participants?
length(unique(dat2$id)) #101
#how many sentences?
length(unique(dat2$sentence)) #21
#Make the trial id column
dat2$trial_id = rep(1:20)
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "Trial_id", id = "id", id.trial = "Trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "id", id.trial = "trial_id",
cutoff = 0, token.split = " ", group.by = "id")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
dat3 = na.omit(dat2)
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
##score the things!
##get better column names
colnames(dat2)[1] = "Sub.id"
##Recheck n
#how many participants?
length(unique(dat2$Sub.id)) #100
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
##Okay, 100 participants each studied 20 sentences
dat3 = na.omit(dat2)
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
####Validate the sentence functions####
library(lrd)
dat = read.csv("AzBio_ScoreTemp_AH.csv")
#isolate the participant ids, sentences, and responses
dat2 = dat[ , c(1, 3, 4)]
#how many participants?
length(unique(dat2$Participant.Private.ID)) #101
#how many sentences?
length(unique(dat2$sentence)) #21
##score the things!
##get better column names
colnames(dat2)[1] = "Sub.id"
##Need to make a trial id
#first, remove the blank sentences
dat2 = subset(dat2,
dat2$sentence != "")
##Recheck n
#how many participants?
length(unique(dat2$Sub.id)) #100
#how many sentences?
length(unique(dat2$sentence)) #20
##Okay, 100 participants each studied 20 sentences
#Make the trial id column
dat2$trial_id = rep(1:20)
dat3 = na.omit(dat2)
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
tapply(dat3$trial_id, dat3$Sub.id, length)
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = "")
cutoff = 0, token.split = "  )
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat3, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
length(unique(dat3$Sub.id))
table(dat3$trial_id)
View(dat2)
dat2$Response[is.na(dat$Response == T)] = " "
dat2[is.na(dat2)] <- " "
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "Sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences = prop_correct_sentence(dat2, responses = "Response", key = "sentence",
key.trial = "trial_id", id = "Sub.id", id.trial = "trial_id",
cutoff = 0, token.split = " ")
scored_sentences$DF_Scored
scored_sentences$DF_Participant
